{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.path.abspath(os.path.dirname(\"__file__\")), '..'))\n",
    "from utils import data_handling, model_development\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model training policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_development.configure_training_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2624, 300, 300), 2624, 2624]\n"
     ]
    }
   ],
   "source": [
    "cell_images, cell_labels, cell_types = data_handling.load_data_from_file('../data/labels.csv')\n",
    "print([cell_images.shape, len(cell_labels), len(cell_types)])\n",
    "\n",
    "# @interact(n = (0, cell_images.shape[0] - 1))\n",
    "# def display_data(n = 0):\n",
    "#     fig, axs = plt.subplots()\n",
    "#     axs.imshow(cell_images[n], cmap = 'gray')\n",
    "#     axs.set_title(f'Class: {cell_labels[n]} - Type: {cell_types[n]}')\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1281, 300, 300, 3), (227, 300, 300, 3)]\n"
     ]
    }
   ],
   "source": [
    "mono_good = data_handling.query_data_by_labels_and_types(data = cell_images, labels = cell_labels, types = cell_types,\n",
    "    filter_by_labels = 0.0, filter_by_types = 'mono')\n",
    "poly_good = data_handling.query_data_by_labels_and_types(data = cell_images, labels = cell_labels, types = cell_types,\n",
    "    filter_by_labels = 0.0, filter_by_types = 'poly')\n",
    "X = np.concatenate((mono_good, poly_good), axis = 0)\n",
    "X = np.stack((X,) * 3, axis = -1)\n",
    "\n",
    "train_test_split = int(0.85 * X.shape[0])\n",
    "X_train, X_test = X[: train_test_split], X[train_test_split :]\n",
    "print([X_train.shape, X_test.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classification model (first phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.4021\n",
      "Epoch 1: val_loss improved from inf to 0.26576, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 92s 1s/step - loss: 0.4021 - val_loss: 0.2658\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 2: val_loss improved from 0.26576 to 0.20845, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1987 - val_loss: 0.2085\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1731\n",
      "Epoch 3: val_loss did not improve from 0.20845\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1731 - val_loss: 0.2567\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1766\n",
      "Epoch 4: val_loss improved from 0.20845 to 0.18074, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1766 - val_loss: 0.1807\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1536\n",
      "Epoch 5: val_loss improved from 0.18074 to 0.17851, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1536 - val_loss: 0.1785\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1525\n",
      "Epoch 6: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1525 - val_loss: 0.1824\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1367\n",
      "Epoch 7: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1367 - val_loss: 0.2663\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1629\n",
      "Epoch 8: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1629 - val_loss: 0.2206\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1265\n",
      "Epoch 9: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1265 - val_loss: 0.1786\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1531\n",
      "Epoch 10: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1531 - val_loss: 0.2951\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1384\n",
      "Epoch 11: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1384 - val_loss: 0.1807\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 12: val_loss did not improve from 0.17851\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1299 - val_loss: 0.2067\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1275\n",
      "Epoch 13: val_loss improved from 0.17851 to 0.17784, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1275 - val_loss: 0.1778\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1264\n",
      "Epoch 14: val_loss improved from 0.17784 to 0.17663, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1264 - val_loss: 0.1766\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 15: val_loss improved from 0.17663 to 0.17492, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1344 - val_loss: 0.1749\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 16: val_loss did not improve from 0.17492\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1127 - val_loss: 0.1777\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 17: val_loss did not improve from 0.17492\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1279 - val_loss: 0.1796\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 18: val_loss improved from 0.17492 to 0.17491, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 83s 1s/step - loss: 0.1196 - val_loss: 0.1749\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 19: val_loss did not improve from 0.17491\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1215 - val_loss: 0.1773\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1360\n",
      "Epoch 20: val_loss did not improve from 0.17491\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1360 - val_loss: 0.1761\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 21: val_loss improved from 0.17491 to 0.17429, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1129 - val_loss: 0.1743\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 22: val_loss did not improve from 0.17429\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1150 - val_loss: 0.1909\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1345\n",
      "Epoch 23: val_loss did not improve from 0.17429\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1345 - val_loss: 0.2364\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 24: val_loss did not improve from 0.17429\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1139 - val_loss: 0.1752\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 25: val_loss did not improve from 0.17429\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1164 - val_loss: 0.1744\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 26: val_loss did not improve from 0.17429\n",
      "68/68 [==============================] - 82s 1s/step - loss: 0.1166 - val_loss: 0.1779\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 27: val_loss improved from 0.17429 to 0.17368, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1101 - val_loss: 0.1737\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1213\n",
      "Epoch 28: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1213 - val_loss: 0.1760\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 29: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1161 - val_loss: 0.1788\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 30: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1064 - val_loss: 0.1831\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 31: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1148 - val_loss: 0.1753\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 32: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1184 - val_loss: 0.1765\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 33: val_loss did not improve from 0.17368\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1240 - val_loss: 0.1756\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 34: val_loss improved from 0.17368 to 0.17270, saving model to ../models/weights\\CAE_VGG19_1.00.00_fs.hdf5\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1066 - val_loss: 0.1727\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 35: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1056 - val_loss: 0.1804\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 36: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1053 - val_loss: 0.1781\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 37: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1038 - val_loss: 0.1752\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 38: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1049 - val_loss: 0.1776\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 39: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1139 - val_loss: 0.1727\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 40: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1138 - val_loss: 0.1744\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 41: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1135 - val_loss: 0.1738\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 42: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1083 - val_loss: 0.1762\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 43: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1026 - val_loss: 0.1771\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 44: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1025 - val_loss: 0.1817\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 45: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1033 - val_loss: 0.1794\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 46: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1081 - val_loss: 0.1761\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 47: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 82s 1s/step - loss: 0.1094 - val_loss: 0.1804\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 48: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1021 - val_loss: 0.1764\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 49: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1010 - val_loss: 0.1768\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 50: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1020 - val_loss: 0.1751\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 51: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1012 - val_loss: 0.1771\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 52: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1009 - val_loss: 0.1771\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 53: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1005 - val_loss: 0.1807\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 54: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.1002 - val_loss: 0.1761\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 55: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0996 - val_loss: 0.1748\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 56: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.1024 - val_loss: 0.1829\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 57: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.0990 - val_loss: 0.1820\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 58: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0993 - val_loss: 0.1792\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 59: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.1040 - val_loss: 0.1805\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 60: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0988 - val_loss: 0.1809\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 61: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0981 - val_loss: 0.1968\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 62: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0980 - val_loss: 0.1852\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 63: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0997 - val_loss: 0.1824\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 64: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0980 - val_loss: 0.1848\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 65: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.0990 - val_loss: 0.1832\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 66: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0970 - val_loss: 0.1804\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 67: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0973 - val_loss: 0.1867\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 68: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0974 - val_loss: 0.1772\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 69: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0975 - val_loss: 0.1861\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 70: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0966 - val_loss: 0.1816\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 71: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0962 - val_loss: 0.1948\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 72: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0982 - val_loss: 0.1849\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 73: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0964 - val_loss: 0.1912\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 74: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0970 - val_loss: 0.1865\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 75: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 81s 1s/step - loss: 0.0959 - val_loss: 0.1935\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0955\n",
      "Epoch 76: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0955 - val_loss: 0.1818\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0951\n",
      "Epoch 77: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0951 - val_loss: 0.1834\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0953\n",
      "Epoch 78: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0953 - val_loss: 0.1831\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 79: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0954 - val_loss: 0.1820\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 80: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0946 - val_loss: 0.1846\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 81: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0948 - val_loss: 0.1856\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 82: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0943 - val_loss: 0.1917\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 83: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0946 - val_loss: 0.1871\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 84: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0942 - val_loss: 0.1860\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 85: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0944 - val_loss: 0.1850\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 86: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0941 - val_loss: 0.1827\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0939\n",
      "Epoch 87: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0939 - val_loss: 0.1841\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0945\n",
      "Epoch 88: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0945 - val_loss: 0.1943\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 89: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0942 - val_loss: 0.1896\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 90: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0934 - val_loss: 0.1840\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 91: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0932 - val_loss: 0.1916\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0936\n",
      "Epoch 92: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0936 - val_loss: 0.1887\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 93: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0933 - val_loss: 0.1864\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 94: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0933 - val_loss: 0.1896\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 95: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0930 - val_loss: 0.1918\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 96: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0927 - val_loss: 0.1900\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 97: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0932 - val_loss: 0.1880\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 98: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0929 - val_loss: 0.1922\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 99: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0929 - val_loss: 0.1896\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 100: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0922 - val_loss: 0.1894\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 101: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0921 - val_loss: 0.1907\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 102: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0920 - val_loss: 0.1868\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0929\n",
      "Epoch 103: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0929 - val_loss: 0.1887\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 104: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0924 - val_loss: 0.1964\n",
      "Epoch 105/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 105: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0921 - val_loss: 0.1903\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0919\n",
      "Epoch 106: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0919 - val_loss: 0.1858\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 107: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0924 - val_loss: 0.1899\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 108: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0920 - val_loss: 0.1863\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 109: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0915 - val_loss: 0.1903\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 110: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0915 - val_loss: 0.1896\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 111: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0920 - val_loss: 0.1869\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 112: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0917 - val_loss: 0.1874\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0912\n",
      "Epoch 113: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 76s 1s/step - loss: 0.0912 - val_loss: 0.1908\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 114: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0915 - val_loss: 0.1877\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0916\n",
      "Epoch 115: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0916 - val_loss: 0.1895\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 116: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0909 - val_loss: 0.1904\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 117: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0911 - val_loss: 0.1954\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0910\n",
      "Epoch 118: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0910 - val_loss: 0.1863\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0910\n",
      "Epoch 119: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0910 - val_loss: 0.1877\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 120: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0909 - val_loss: 0.1939\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 121: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0903 - val_loss: 0.1906\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 122: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0909 - val_loss: 0.1888\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0904\n",
      "Epoch 123: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0904 - val_loss: 0.1882\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 124: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0905 - val_loss: 0.1895\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0907\n",
      "Epoch 125: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0907 - val_loss: 0.1980\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 126: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0909 - val_loss: 0.1908\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 127: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0903 - val_loss: 0.1937\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0904\n",
      "Epoch 128: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0904 - val_loss: 0.1866\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 129: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0903 - val_loss: 0.1928\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0901\n",
      "Epoch 130: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0901 - val_loss: 0.1867\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 131: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0903 - val_loss: 0.1926\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 132: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0897 - val_loss: 0.1908\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 133: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0906 - val_loss: 0.1917\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 134: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0911 - val_loss: 0.1919\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 135: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0898 - val_loss: 0.1932\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 136: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0898 - val_loss: 0.1907\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 137: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0895 - val_loss: 0.1888\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 138: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0897 - val_loss: 0.1899\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 139: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0897 - val_loss: 0.1879\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 140: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0898 - val_loss: 0.1911\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 141: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0891 - val_loss: 0.1964\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 142: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0894 - val_loss: 0.1929\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 143: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0895 - val_loss: 0.1897\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 144: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0893 - val_loss: 0.1901\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 145: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0891 - val_loss: 0.1890\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 146: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0893 - val_loss: 0.1891\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0892\n",
      "Epoch 147: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0892 - val_loss: 0.1912\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 148: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0890 - val_loss: 0.1927\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 149: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0894 - val_loss: 0.1956\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 150: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0893 - val_loss: 0.1945\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 151: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0890 - val_loss: 0.1890\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 152: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0888 - val_loss: 0.1958\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 153: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0890 - val_loss: 0.1914\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 154: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0893 - val_loss: 0.1987\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 155: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0887 - val_loss: 0.1942\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0889\n",
      "Epoch 156: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0889 - val_loss: 0.1922\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 157: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0888 - val_loss: 0.1917\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0886\n",
      "Epoch 158: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0886 - val_loss: 0.1946\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 159: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0888 - val_loss: 0.1987\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 160: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0885 - val_loss: 0.1958\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 161: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0888 - val_loss: 0.1942\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 162: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0884 - val_loss: 0.1956\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 163: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0887 - val_loss: 0.1905\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 164: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0884 - val_loss: 0.1943\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 165: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0884 - val_loss: 0.1927\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 166: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0885 - val_loss: 0.1954\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 167: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0885 - val_loss: 0.1895\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 168: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0882 - val_loss: 0.1947\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0883\n",
      "Epoch 169: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0883 - val_loss: 0.1946\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 170: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0882 - val_loss: 0.1915\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 171: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0884 - val_loss: 0.1914\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 172: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0890 - val_loss: 0.1956\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 173: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0887 - val_loss: 0.1954\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0883\n",
      "Epoch 174: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0883 - val_loss: 0.1924\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 175: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0884 - val_loss: 0.1893\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0883\n",
      "Epoch 176: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0883 - val_loss: 0.2017\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 177: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0880 - val_loss: 0.1932\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 178: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0880 - val_loss: 0.1956\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 179: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0882 - val_loss: 0.1921\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 180: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0882 - val_loss: 0.1975\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 181: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0880 - val_loss: 0.1987\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 182: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0881 - val_loss: 0.1966\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 183: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0880 - val_loss: 0.1957\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 184: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0879 - val_loss: 0.1965\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 185: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0878 - val_loss: 0.1976\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 186: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0879 - val_loss: 0.1969\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 187: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 76s 1s/step - loss: 0.0877 - val_loss: 0.1945\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0876\n",
      "Epoch 188: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0876 - val_loss: 0.1968\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0876\n",
      "Epoch 189: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0876 - val_loss: 0.1958\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0876\n",
      "Epoch 190: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 76s 1s/step - loss: 0.0876 - val_loss: 0.1966\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0875\n",
      "Epoch 191: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0875 - val_loss: 0.1998\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 192: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0878 - val_loss: 0.1994\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 193: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0874 - val_loss: 0.1989\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 194: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0874 - val_loss: 0.1972\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 195: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 78s 1s/step - loss: 0.0874 - val_loss: 0.1998\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0873\n",
      "Epoch 196: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0873 - val_loss: 0.1963\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0873\n",
      "Epoch 197: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 82s 1s/step - loss: 0.0873 - val_loss: 0.2008\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0876\n",
      "Epoch 198: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 79s 1s/step - loss: 0.0876 - val_loss: 0.2000\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 199: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 77s 1s/step - loss: 0.0872 - val_loss: 0.1995\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 200: val_loss did not improve from 0.17270\n",
      "68/68 [==============================] - 80s 1s/step - loss: 0.0877 - val_loss: 0.1976\n"
     ]
    }
   ],
   "source": [
    "optimizer = model_development.create_optimizer('nadam')\n",
    "cae_vgg19 = \\\n",
    "    model_development.cae_VGG19(input_shape = (300, 300, 3), weights = 'imagenet', freeze_convolutional_base = True, display_model_information = False)\n",
    "\n",
    "history, training_time = model_development.train_reconstruction_model(1, cae_vgg19, optimizer = optimizer, model_name = 'CAE_VGG19', version = '1.00.00_fs',\n",
    "    X = X_train, Y = X_train, \n",
    "    metric_to_monitor = 'val_loss', n_of_epochs = 200, batch_size = 16, validation_split_ratio = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classification model (second phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 1: val_loss improved from inf to 0.17042, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 148s 1s/step - loss: 0.0997 - val_loss: 0.1704\n",
      "Epoch 2/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 2: val_loss did not improve from 0.17042\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0930 - val_loss: 0.1715\n",
      "Epoch 3/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 3: val_loss improved from 0.17042 to 0.16637, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0903 - val_loss: 0.1664\n",
      "Epoch 4/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 4: val_loss improved from 0.16637 to 0.16140, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0891 - val_loss: 0.1614\n",
      "Epoch 5/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0878\n",
      "Epoch 5: val_loss improved from 0.16140 to 0.16032, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0878 - val_loss: 0.1603\n",
      "Epoch 6/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0870\n",
      "Epoch 6: val_loss improved from 0.16032 to 0.15784, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0870 - val_loss: 0.1578\n",
      "Epoch 7/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0867\n",
      "Epoch 7: val_loss did not improve from 0.15784\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0867 - val_loss: 0.1583\n",
      "Epoch 8/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0858\n",
      "Epoch 8: val_loss improved from 0.15784 to 0.15565, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0858 - val_loss: 0.1556\n",
      "Epoch 9/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0855\n",
      "Epoch 9: val_loss improved from 0.15565 to 0.15557, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 144s 1s/step - loss: 0.0855 - val_loss: 0.1556\n",
      "Epoch 10/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0850\n",
      "Epoch 10: val_loss did not improve from 0.15557\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0850 - val_loss: 0.1564\n",
      "Epoch 11/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0848\n",
      "Epoch 11: val_loss improved from 0.15557 to 0.15283, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0848 - val_loss: 0.1528\n",
      "Epoch 12/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0844\n",
      "Epoch 12: val_loss improved from 0.15283 to 0.15264, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 141s 1s/step - loss: 0.0844 - val_loss: 0.1526\n",
      "Epoch 13/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 13: val_loss did not improve from 0.15264\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0841 - val_loss: 0.1532\n",
      "Epoch 14/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0840\n",
      "Epoch 14: val_loss improved from 0.15264 to 0.15196, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0840 - val_loss: 0.1520\n",
      "Epoch 15/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0835\n",
      "Epoch 15: val_loss did not improve from 0.15196\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0835 - val_loss: 0.1524\n",
      "Epoch 16/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0833\n",
      "Epoch 16: val_loss improved from 0.15196 to 0.15150, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0833 - val_loss: 0.1515\n",
      "Epoch 17/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0831\n",
      "Epoch 17: val_loss improved from 0.15150 to 0.15056, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0831 - val_loss: 0.1506\n",
      "Epoch 18/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0828\n",
      "Epoch 18: val_loss did not improve from 0.15056\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0828 - val_loss: 0.1516\n",
      "Epoch 19/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0826\n",
      "Epoch 19: val_loss did not improve from 0.15056\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0826 - val_loss: 0.1518\n",
      "Epoch 20/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 20: val_loss did not improve from 0.15056\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0823 - val_loss: 0.1506\n",
      "Epoch 21/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0821\n",
      "Epoch 21: val_loss did not improve from 0.15056\n",
      "136/136 [==============================] - 141s 1s/step - loss: 0.0821 - val_loss: 0.1521\n",
      "Epoch 22/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0818\n",
      "Epoch 22: val_loss improved from 0.15056 to 0.14928, saving model to ../models/weights\\CAE_VGG19_1.00.00_ss.hdf5\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0818 - val_loss: 0.1493\n",
      "Epoch 23/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0818\n",
      "Epoch 23: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0818 - val_loss: 0.1514\n",
      "Epoch 24/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0815\n",
      "Epoch 24: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0815 - val_loss: 0.1516\n",
      "Epoch 25/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0812\n",
      "Epoch 25: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0812 - val_loss: 0.1512\n",
      "Epoch 26/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0812\n",
      "Epoch 26: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0812 - val_loss: 0.1528\n",
      "Epoch 27/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0811\n",
      "Epoch 27: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0811 - val_loss: 0.1528\n",
      "Epoch 28/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0809\n",
      "Epoch 28: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0809 - val_loss: 0.1518\n",
      "Epoch 29/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0809\n",
      "Epoch 29: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0809 - val_loss: 0.1528\n",
      "Epoch 30/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0806\n",
      "Epoch 30: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0806 - val_loss: 0.1507\n",
      "Epoch 31/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0804\n",
      "Epoch 31: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0804 - val_loss: 0.1522\n",
      "Epoch 32/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 32: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0803 - val_loss: 0.1527\n",
      "Epoch 33/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 33: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0803 - val_loss: 0.1534\n",
      "Epoch 34/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 34: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0802 - val_loss: 0.1520\n",
      "Epoch 35/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 35: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 140s 1s/step - loss: 0.0802 - val_loss: 0.1531\n",
      "Epoch 36/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0800\n",
      "Epoch 36: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0800 - val_loss: 0.1523\n",
      "Epoch 37/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0798\n",
      "Epoch 37: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0798 - val_loss: 0.1544\n",
      "Epoch 38/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0798\n",
      "Epoch 38: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0798 - val_loss: 0.1542\n",
      "Epoch 39/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0797\n",
      "Epoch 39: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0797 - val_loss: 0.1542\n",
      "Epoch 40/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0794\n",
      "Epoch 40: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0794 - val_loss: 0.1535\n",
      "Epoch 41/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0795\n",
      "Epoch 41: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0795 - val_loss: 0.1538\n",
      "Epoch 42/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0792\n",
      "Epoch 42: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0792 - val_loss: 0.1540\n",
      "Epoch 43/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0793\n",
      "Epoch 43: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0793 - val_loss: 0.1543\n",
      "Epoch 44/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0792\n",
      "Epoch 44: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0792 - val_loss: 0.1536\n",
      "Epoch 45/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0792\n",
      "Epoch 45: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0792 - val_loss: 0.1539\n",
      "Epoch 46/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0790\n",
      "Epoch 46: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0790 - val_loss: 0.1544\n",
      "Epoch 47/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0788\n",
      "Epoch 47: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0788 - val_loss: 0.1546\n",
      "Epoch 48/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 48: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0789 - val_loss: 0.1568\n",
      "Epoch 49/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 49: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0789 - val_loss: 0.1545\n",
      "Epoch 50/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0788\n",
      "Epoch 50: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0788 - val_loss: 0.1559\n",
      "Epoch 51/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0789\n",
      "Epoch 51: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0789 - val_loss: 0.1558\n",
      "Epoch 52/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 52: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0785 - val_loss: 0.1541\n",
      "Epoch 53/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 53: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0787 - val_loss: 0.1554\n",
      "Epoch 54/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 54: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 141s 1s/step - loss: 0.0785 - val_loss: 0.1555\n",
      "Epoch 55/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 55: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0783 - val_loss: 0.1559\n",
      "Epoch 56/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 56: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0783 - val_loss: 0.1551\n",
      "Epoch 57/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 57: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0783 - val_loss: 0.1548\n",
      "Epoch 58/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 58: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0783 - val_loss: 0.1561\n",
      "Epoch 59/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 59: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0782 - val_loss: 0.1556\n",
      "Epoch 60/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 60: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0780 - val_loss: 0.1548\n",
      "Epoch 61/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 61: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0780 - val_loss: 0.1554\n",
      "Epoch 62/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 62: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0780 - val_loss: 0.1578\n",
      "Epoch 63/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 63: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0780 - val_loss: 0.1550\n",
      "Epoch 64/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 64: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0779 - val_loss: 0.1572\n",
      "Epoch 65/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0778\n",
      "Epoch 65: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0778 - val_loss: 0.1568\n",
      "Epoch 66/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 66: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0777 - val_loss: 0.1558\n",
      "Epoch 67/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 67: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0777 - val_loss: 0.1556\n",
      "Epoch 68/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 68: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0774 - val_loss: 0.1560\n",
      "Epoch 69/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 69: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0776 - val_loss: 0.1567\n",
      "Epoch 70/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 70: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0776 - val_loss: 0.1553\n",
      "Epoch 71/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 71: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0773 - val_loss: 0.1556\n",
      "Epoch 72/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0775\n",
      "Epoch 72: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0775 - val_loss: 0.1557\n",
      "Epoch 73/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 73: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0773 - val_loss: 0.1577\n",
      "Epoch 74/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 74: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0772 - val_loss: 0.1558\n",
      "Epoch 75/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 75: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0772 - val_loss: 0.1564\n",
      "Epoch 76/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 76: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0770 - val_loss: 0.1558\n",
      "Epoch 77/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 77: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0770 - val_loss: 0.1556\n",
      "Epoch 78/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 78: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0770 - val_loss: 0.1555\n",
      "Epoch 79/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 79: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0770 - val_loss: 0.1547\n",
      "Epoch 80/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 80: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0768 - val_loss: 0.1547\n",
      "Epoch 81/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 81: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0768 - val_loss: 0.1565\n",
      "Epoch 82/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 82: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0768 - val_loss: 0.1565\n",
      "Epoch 83/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0767\n",
      "Epoch 83: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 139s 1s/step - loss: 0.0767 - val_loss: 0.1547\n",
      "Epoch 84/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0766\n",
      "Epoch 84: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0766 - val_loss: 0.1567\n",
      "Epoch 85/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0765\n",
      "Epoch 85: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0765 - val_loss: 0.1574\n",
      "Epoch 86/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0766\n",
      "Epoch 86: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 996ms/step - loss: 0.0766 - val_loss: 0.1577\n",
      "Epoch 87/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0767\n",
      "Epoch 87: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0767 - val_loss: 0.1544\n",
      "Epoch 88/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0765\n",
      "Epoch 88: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0765 - val_loss: 0.1568\n",
      "Epoch 89/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0764\n",
      "Epoch 89: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0764 - val_loss: 0.1557\n",
      "Epoch 90/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0763\n",
      "Epoch 90: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0763 - val_loss: 0.1569\n",
      "Epoch 91/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0764\n",
      "Epoch 91: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0764 - val_loss: 0.1554\n",
      "Epoch 92/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0763\n",
      "Epoch 92: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0763 - val_loss: 0.1565\n",
      "Epoch 93/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0761\n",
      "Epoch 93: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0761 - val_loss: 0.1593\n",
      "Epoch 94/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0763\n",
      "Epoch 94: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0763 - val_loss: 0.1590\n",
      "Epoch 95/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0761\n",
      "Epoch 95: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0761 - val_loss: 0.1564\n",
      "Epoch 96/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0761\n",
      "Epoch 96: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0761 - val_loss: 0.1576\n",
      "Epoch 97/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 97: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0759 - val_loss: 0.1572\n",
      "Epoch 98/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 98: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0759 - val_loss: 0.1588\n",
      "Epoch 99/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0758\n",
      "Epoch 99: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0758 - val_loss: 0.1575\n",
      "Epoch 100/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0759\n",
      "Epoch 100: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0759 - val_loss: 0.1573\n",
      "Epoch 101/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0757\n",
      "Epoch 101: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0757 - val_loss: 0.1565\n",
      "Epoch 102/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0757\n",
      "Epoch 102: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0757 - val_loss: 0.1585\n",
      "Epoch 103/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0757\n",
      "Epoch 103: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0757 - val_loss: 0.1583\n",
      "Epoch 104/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0756\n",
      "Epoch 104: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0756 - val_loss: 0.1585\n",
      "Epoch 105/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0751\n",
      "Epoch 105: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0751 - val_loss: 0.1576\n",
      "Epoch 106/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0752\n",
      "Epoch 106: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 996ms/step - loss: 0.0752 - val_loss: 0.1615\n",
      "Epoch 107/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0754\n",
      "Epoch 107: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0754 - val_loss: 0.1619\n",
      "Epoch 108/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0753\n",
      "Epoch 108: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0753 - val_loss: 0.1603\n",
      "Epoch 109/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0751\n",
      "Epoch 109: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0751 - val_loss: 0.1619\n",
      "Epoch 110/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0753\n",
      "Epoch 110: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0753 - val_loss: 0.1592\n",
      "Epoch 111/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0752\n",
      "Epoch 111: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0752 - val_loss: 0.1601\n",
      "Epoch 112/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0750\n",
      "Epoch 112: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0750 - val_loss: 0.1588\n",
      "Epoch 113/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0750\n",
      "Epoch 113: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0750 - val_loss: 0.1587\n",
      "Epoch 114/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 114: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0749 - val_loss: 0.1606\n",
      "Epoch 115/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0746\n",
      "Epoch 115: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0746 - val_loss: 0.1585\n",
      "Epoch 116/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0747\n",
      "Epoch 116: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0747 - val_loss: 0.1634\n",
      "Epoch 117/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0746\n",
      "Epoch 117: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0746 - val_loss: 0.1645\n",
      "Epoch 118/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0748\n",
      "Epoch 118: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0748 - val_loss: 0.1604\n",
      "Epoch 119/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0745\n",
      "Epoch 119: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0745 - val_loss: 0.1633\n",
      "Epoch 120/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0743\n",
      "Epoch 120: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0743 - val_loss: 0.1615\n",
      "Epoch 121/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0743\n",
      "Epoch 121: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0743 - val_loss: 0.1606\n",
      "Epoch 122/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0746\n",
      "Epoch 122: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0746 - val_loss: 0.1608\n",
      "Epoch 123/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0744\n",
      "Epoch 123: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0744 - val_loss: 0.1636\n",
      "Epoch 124/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0742\n",
      "Epoch 124: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0742 - val_loss: 0.1603\n",
      "Epoch 125/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0740\n",
      "Epoch 125: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0740 - val_loss: 0.1626\n",
      "Epoch 126/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0740\n",
      "Epoch 126: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 138s 1s/step - loss: 0.0740 - val_loss: 0.1610\n",
      "Epoch 127/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0740\n",
      "Epoch 127: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0740 - val_loss: 0.1652\n",
      "Epoch 128/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0740\n",
      "Epoch 128: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0740 - val_loss: 0.1606\n",
      "Epoch 129/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0739\n",
      "Epoch 129: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0739 - val_loss: 0.1583\n",
      "Epoch 130/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0739\n",
      "Epoch 130: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0739 - val_loss: 0.1633\n",
      "Epoch 131/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0739\n",
      "Epoch 131: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0739 - val_loss: 0.1628\n",
      "Epoch 132/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0736\n",
      "Epoch 132: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0736 - val_loss: 0.1645\n",
      "Epoch 133/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0737\n",
      "Epoch 133: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0737 - val_loss: 0.1607\n",
      "Epoch 134/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0736\n",
      "Epoch 134: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0736 - val_loss: 0.1639\n",
      "Epoch 135/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0736\n",
      "Epoch 135: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0736 - val_loss: 0.1658\n",
      "Epoch 136/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0736\n",
      "Epoch 136: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0736 - val_loss: 0.1637\n",
      "Epoch 137/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0733\n",
      "Epoch 137: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 993ms/step - loss: 0.0733 - val_loss: 0.1644\n",
      "Epoch 138/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0735\n",
      "Epoch 138: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0735 - val_loss: 0.1645\n",
      "Epoch 139/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0733\n",
      "Epoch 139: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0733 - val_loss: 0.1622\n",
      "Epoch 140/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0732\n",
      "Epoch 140: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0732 - val_loss: 0.1648\n",
      "Epoch 141/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0733\n",
      "Epoch 141: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0733 - val_loss: 0.1642\n",
      "Epoch 142/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0732\n",
      "Epoch 142: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0732 - val_loss: 0.1650\n",
      "Epoch 143/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0730\n",
      "Epoch 143: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 996ms/step - loss: 0.0730 - val_loss: 0.1690\n",
      "Epoch 144/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0731\n",
      "Epoch 144: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0731 - val_loss: 0.1616\n",
      "Epoch 145/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0735\n",
      "Epoch 145: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0735 - val_loss: 0.1653\n",
      "Epoch 146/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0729\n",
      "Epoch 146: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0729 - val_loss: 0.1692\n",
      "Epoch 147/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0729\n",
      "Epoch 147: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0729 - val_loss: 0.1664\n",
      "Epoch 148/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0730\n",
      "Epoch 148: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0730 - val_loss: 0.1655\n",
      "Epoch 149/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0727\n",
      "Epoch 149: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0727 - val_loss: 0.1639\n",
      "Epoch 150/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 150: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0726 - val_loss: 0.1655\n",
      "Epoch 151/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0728\n",
      "Epoch 151: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0728 - val_loss: 0.1659\n",
      "Epoch 152/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 152: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 997ms/step - loss: 0.0726 - val_loss: 0.1653\n",
      "Epoch 153/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0728\n",
      "Epoch 153: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0728 - val_loss: 0.1670\n",
      "Epoch 154/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 154: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0726 - val_loss: 0.1669\n",
      "Epoch 155/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0725\n",
      "Epoch 155: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0725 - val_loss: 0.1632\n",
      "Epoch 156/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0724\n",
      "Epoch 156: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0724 - val_loss: 0.1696\n",
      "Epoch 157/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0724\n",
      "Epoch 157: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0724 - val_loss: 0.1670\n",
      "Epoch 158/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0722\n",
      "Epoch 158: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0722 - val_loss: 0.1633\n",
      "Epoch 159/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0723\n",
      "Epoch 159: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 996ms/step - loss: 0.0723 - val_loss: 0.1639\n",
      "Epoch 160/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0724\n",
      "Epoch 160: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0724 - val_loss: 0.1640\n",
      "Epoch 161/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0726\n",
      "Epoch 161: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 994ms/step - loss: 0.0726 - val_loss: 0.1623\n",
      "Epoch 162/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0720\n",
      "Epoch 162: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0720 - val_loss: 0.1629\n",
      "Epoch 163/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0725\n",
      "Epoch 163: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0725 - val_loss: 0.1619\n",
      "Epoch 164/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0721\n",
      "Epoch 164: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0721 - val_loss: 0.1744\n",
      "Epoch 165/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0720\n",
      "Epoch 165: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0720 - val_loss: 0.1694\n",
      "Epoch 166/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0719\n",
      "Epoch 166: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0719 - val_loss: 0.1678\n",
      "Epoch 167/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0719\n",
      "Epoch 167: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0719 - val_loss: 0.1625\n",
      "Epoch 168/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0722\n",
      "Epoch 168: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0722 - val_loss: 0.1655\n",
      "Epoch 169/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0720\n",
      "Epoch 169: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0720 - val_loss: 0.1651\n",
      "Epoch 170/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 170: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0717 - val_loss: 0.1661\n",
      "Epoch 171/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0720\n",
      "Epoch 171: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0720 - val_loss: 0.1632\n",
      "Epoch 172/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 172: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0717 - val_loss: 0.1697\n",
      "Epoch 173/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0719\n",
      "Epoch 173: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 994ms/step - loss: 0.0719 - val_loss: 0.1701\n",
      "Epoch 174/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 174: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0717 - val_loss: 0.1681\n",
      "Epoch 175/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0718\n",
      "Epoch 175: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0718 - val_loss: 0.1660\n",
      "Epoch 176/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0717\n",
      "Epoch 176: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0717 - val_loss: 0.1679\n",
      "Epoch 177/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0716\n",
      "Epoch 177: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 996ms/step - loss: 0.0716 - val_loss: 0.1698\n",
      "Epoch 178/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0713\n",
      "Epoch 178: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0713 - val_loss: 0.1697\n",
      "Epoch 179/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0715\n",
      "Epoch 179: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0715 - val_loss: 0.1700\n",
      "Epoch 180/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 180: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 994ms/step - loss: 0.0714 - val_loss: 0.1663\n",
      "Epoch 181/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0715\n",
      "Epoch 181: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1000ms/step - loss: 0.0715 - val_loss: 0.1700\n",
      "Epoch 182/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0712\n",
      "Epoch 182: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0712 - val_loss: 0.1723\n",
      "Epoch 183/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0714\n",
      "Epoch 183: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0714 - val_loss: 0.1691\n",
      "Epoch 184/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0711\n",
      "Epoch 184: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0711 - val_loss: 0.1642\n",
      "Epoch 185/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0711\n",
      "Epoch 185: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0711 - val_loss: 0.1666\n",
      "Epoch 186/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0712\n",
      "Epoch 186: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0712 - val_loss: 0.1672\n",
      "Epoch 187/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0711\n",
      "Epoch 187: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0711 - val_loss: 0.1631\n",
      "Epoch 188/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 188: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0710 - val_loss: 0.1675\n",
      "Epoch 189/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0708\n",
      "Epoch 189: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0708 - val_loss: 0.1669\n",
      "Epoch 190/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 190: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 999ms/step - loss: 0.0710 - val_loss: 0.1662\n",
      "Epoch 191/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0709\n",
      "Epoch 191: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 991ms/step - loss: 0.0709 - val_loss: 0.1660\n",
      "Epoch 192/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 192: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0710 - val_loss: 0.1714\n",
      "Epoch 193/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0709\n",
      "Epoch 193: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 137s 1s/step - loss: 0.0709 - val_loss: 0.1701\n",
      "Epoch 194/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 194: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 1s/step - loss: 0.0710 - val_loss: 0.1676\n",
      "Epoch 195/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0706\n",
      "Epoch 195: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0706 - val_loss: 0.1724\n",
      "Epoch 196/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0707\n",
      "Epoch 196: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 997ms/step - loss: 0.0707 - val_loss: 0.1684\n",
      "Epoch 197/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0705\n",
      "Epoch 197: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 136s 998ms/step - loss: 0.0705 - val_loss: 0.1701\n",
      "Epoch 198/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0706\n",
      "Epoch 198: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0706 - val_loss: 0.1678\n",
      "Epoch 199/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0706\n",
      "Epoch 199: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0706 - val_loss: 0.1705\n",
      "Epoch 200/200\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0708\n",
      "Epoch 200: val_loss did not improve from 0.14928\n",
      "136/136 [==============================] - 135s 995ms/step - loss: 0.0708 - val_loss: 0.1672\n"
     ]
    }
   ],
   "source": [
    "optimizer = model_development.create_optimizer('adam')\n",
    "history, training_time = \\\n",
    "    model_development.train_reconstruction_model(2, '../models/weights/CAE_VGG19_1.00.00_fs.hdf5', optimizer = optimizer, \n",
    "    model_name = 'CAE_VGG19', version = '1.00.00_ss',\n",
    "    X = X_train, Y = X_train, \n",
    "    metric_to_monitor = 'val_loss', n_of_epochs = 200, batch_size = 8, validation_split_ratio = 0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorflow_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d1c19e37e5f6196bc516d7b965f651cc74a678b65e0e3f127f9e9132621bafc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
