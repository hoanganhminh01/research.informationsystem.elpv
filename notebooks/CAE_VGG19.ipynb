{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.path.abspath(os.path.dirname(\"__file__\")), '..'))\n",
    "from utils import data_handling, model_development\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Configure the model training policy:\n",
    "#\n",
    "model_development.configure_training_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2624, 300, 300), 2624, 2624]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e771e163fe54ff183c300c1579b6d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=2623), Output()), _dom_classes=('widget-interactâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Load and preprocess data:\n",
    "#\n",
    "cell_images, cell_labels, cell_types = data_handling.load_data_from_file('../data/labels.csv', True)\n",
    "print([cell_images.shape, len(cell_labels), len(cell_types)])\n",
    "\n",
    "@interact(n = (0, cell_images.shape[0] - 1))\n",
    "def display_data(n = 0):\n",
    "    fig, axs = plt.subplots()\n",
    "    axs.imshow(cell_images[n], cmap = 'gray')\n",
    "    axs.set_title(f'Class: {cell_labels[n]} - Type: {cell_types[n]}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Prepare data for model training:\n",
    "#\n",
    "mono_good = data_handling.query_data_by_labels_and_types(data = cell_images, labels = cell_labels, types = cell_types,\n",
    "    filter_by_labels = 0.0, filter_by_types = 'mono')\n",
    "poly_good = data_handling.query_data_by_labels_and_types(data = cell_images, labels = cell_labels, types = cell_types,\n",
    "    filter_by_labels = 0.0, filter_by_types = 'poly')\n",
    "X_train = np.concatenate((mono_good, poly_good), axis = 0)\n",
    "X_train = np.stack((X_train,) * 3, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.8343\n",
      "Epoch 1: val_loss improved from inf to 0.54437, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 102s 1s/step - loss: 0.8343 - val_loss: 0.5444\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.4516\n",
      "Epoch 2: val_loss improved from 0.54437 to 0.35775, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 81s 1s/step - loss: 0.4516 - val_loss: 0.3578\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3418\n",
      "Epoch 3: val_loss improved from 0.35775 to 0.31484, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 83s 1s/step - loss: 0.3418 - val_loss: 0.3148\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3601\n",
      "Epoch 4: val_loss improved from 0.31484 to 0.29588, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.3601 - val_loss: 0.2959\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3306\n",
      "Epoch 5: val_loss did not improve from 0.29588\n",
      "76/76 [==============================] - 99s 1s/step - loss: 0.3306 - val_loss: 0.3047\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3165\n",
      "Epoch 6: val_loss did not improve from 0.29588\n",
      "76/76 [==============================] - 97s 1s/step - loss: 0.3165 - val_loss: 0.3133\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2609\n",
      "Epoch 7: val_loss did not improve from 0.29588\n",
      "76/76 [==============================] - 90s 1s/step - loss: 0.2609 - val_loss: 0.3211\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2707\n",
      "Epoch 8: val_loss did not improve from 0.29588\n",
      "76/76 [==============================] - 84s 1s/step - loss: 0.2707 - val_loss: 0.3207\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2655\n",
      "Epoch 9: val_loss did not improve from 0.29588\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2655 - val_loss: 0.3115\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2842\n",
      "Epoch 10: val_loss improved from 0.29588 to 0.29453, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2842 - val_loss: 0.2945\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2948\n",
      "Epoch 11: val_loss did not improve from 0.29453\n",
      "76/76 [==============================] - 84s 1s/step - loss: 0.2948 - val_loss: 0.3679\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.3003\n",
      "Epoch 12: val_loss did not improve from 0.29453\n",
      "76/76 [==============================] - 100s 1s/step - loss: 0.3003 - val_loss: 0.3161\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2474\n",
      "Epoch 13: val_loss did not improve from 0.29453\n",
      "76/76 [==============================] - 97s 1s/step - loss: 0.2474 - val_loss: 0.3708\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2513\n",
      "Epoch 14: val_loss improved from 0.29453 to 0.29155, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2513 - val_loss: 0.2916\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2739\n",
      "Epoch 15: val_loss improved from 0.29155 to 0.28216, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2739 - val_loss: 0.2822\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2735\n",
      "Epoch 16: val_loss did not improve from 0.28216\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2735 - val_loss: 0.3300\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2587\n",
      "Epoch 17: val_loss did not improve from 0.28216\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2587 - val_loss: 0.2851\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2518\n",
      "Epoch 18: val_loss did not improve from 0.28216\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2518 - val_loss: 0.2846\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2595\n",
      "Epoch 19: val_loss did not improve from 0.28216\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2595 - val_loss: 0.2884\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2529\n",
      "Epoch 20: val_loss improved from 0.28216 to 0.27945, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2529 - val_loss: 0.2795\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2378\n",
      "Epoch 21: val_loss did not improve from 0.27945\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2378 - val_loss: 0.2971\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2511\n",
      "Epoch 22: val_loss did not improve from 0.27945\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2511 - val_loss: 0.2876\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2528\n",
      "Epoch 23: val_loss did not improve from 0.27945\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2528 - val_loss: 0.2964\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2388\n",
      "Epoch 24: val_loss improved from 0.27945 to 0.27694, saving model to ../models/weights\\CAE_VGG19_1.00.00.hdf5\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.2388 - val_loss: 0.2769\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2416\n",
      "Epoch 25: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2416 - val_loss: 0.3833\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2218\n",
      "Epoch 26: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2218 - val_loss: 0.2942\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2321\n",
      "Epoch 27: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2321 - val_loss: 0.3005\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2344\n",
      "Epoch 28: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2344 - val_loss: 0.2935\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2309\n",
      "Epoch 29: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2309 - val_loss: 0.2909\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2339\n",
      "Epoch 30: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2339 - val_loss: 0.2892\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2147\n",
      "Epoch 31: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2147 - val_loss: 0.2901\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2251\n",
      "Epoch 32: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2251 - val_loss: 0.2911\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2360\n",
      "Epoch 33: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2360 - val_loss: 0.2781\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2220\n",
      "Epoch 34: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.2220 - val_loss: 0.2828\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2254\n",
      "Epoch 35: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2254 - val_loss: 0.3337\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2093\n",
      "Epoch 36: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2093 - val_loss: 0.2888\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2240\n",
      "Epoch 37: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2240 - val_loss: 0.2837\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2220\n",
      "Epoch 38: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.2220 - val_loss: 0.2786\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2135\n",
      "Epoch 39: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2135 - val_loss: 0.2817\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2075\n",
      "Epoch 40: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2075 - val_loss: 0.2937\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2187\n",
      "Epoch 41: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2187 - val_loss: 0.2807\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2130\n",
      "Epoch 42: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2130 - val_loss: 0.2918\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2050\n",
      "Epoch 43: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2050 - val_loss: 0.2866\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2163\n",
      "Epoch 44: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2163 - val_loss: 0.2919\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2162\n",
      "Epoch 45: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2162 - val_loss: 0.2789\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2059\n",
      "Epoch 46: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2059 - val_loss: 0.3015\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2033\n",
      "Epoch 47: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2033 - val_loss: 0.2971\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 48: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2063 - val_loss: 0.2830\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2028\n",
      "Epoch 49: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2028 - val_loss: 0.2892\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2029\n",
      "Epoch 50: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2029 - val_loss: 0.2953\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2013\n",
      "Epoch 51: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2013 - val_loss: 0.2962\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2034\n",
      "Epoch 52: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2034 - val_loss: 0.2875\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1997\n",
      "Epoch 53: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1997 - val_loss: 0.2997\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2006\n",
      "Epoch 54: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2006 - val_loss: 0.2921\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1994\n",
      "Epoch 55: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1994 - val_loss: 0.2853\n",
      "Epoch 56/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2001\n",
      "Epoch 56: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.2001 - val_loss: 0.2857\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.2009\n",
      "Epoch 57: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.2009 - val_loss: 0.2902\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1986\n",
      "Epoch 58: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1986 - val_loss: 0.2963\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1980\n",
      "Epoch 59: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1980 - val_loss: 0.2874\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1988\n",
      "Epoch 60: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.1988 - val_loss: 0.2994\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1968\n",
      "Epoch 61: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 89s 1s/step - loss: 0.1968 - val_loss: 0.2825\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1969\n",
      "Epoch 62: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 88s 1s/step - loss: 0.1969 - val_loss: 0.3004\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1990\n",
      "Epoch 63: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.1990 - val_loss: 0.2841\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1973\n",
      "Epoch 64: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 87s 1s/step - loss: 0.1973 - val_loss: 0.2952\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1966\n",
      "Epoch 65: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1966 - val_loss: 0.2904\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1972\n",
      "Epoch 66: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1972 - val_loss: 0.2915\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1954\n",
      "Epoch 67: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1954 - val_loss: 0.2878\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1955\n",
      "Epoch 68: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1955 - val_loss: 0.2921\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1946\n",
      "Epoch 69: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1946 - val_loss: 0.2935\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1945\n",
      "Epoch 70: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1945 - val_loss: 0.2829\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1959\n",
      "Epoch 71: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.1959 - val_loss: 0.2918\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1936\n",
      "Epoch 72: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1936 - val_loss: 0.2906\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1939\n",
      "Epoch 73: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1939 - val_loss: 0.2906\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1939\n",
      "Epoch 74: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1939 - val_loss: 0.2977\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1932\n",
      "Epoch 75: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1932 - val_loss: 0.2937\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1927\n",
      "Epoch 76: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1927 - val_loss: 0.2924\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1929\n",
      "Epoch 77: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1929 - val_loss: 0.2911\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1922\n",
      "Epoch 78: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1922 - val_loss: 0.2982\n",
      "Epoch 79/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1923\n",
      "Epoch 79: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 85s 1s/step - loss: 0.1923 - val_loss: 0.2992\n",
      "Epoch 80/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1919\n",
      "Epoch 80: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1919 - val_loss: 0.2964\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 81: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 86s 1s/step - loss: 0.1915 - val_loss: 0.2895\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1914\n",
      "Epoch 82: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 104s 1s/step - loss: 0.1914 - val_loss: 0.2909\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 83: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1915 - val_loss: 0.2871\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1910\n",
      "Epoch 84: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 117s 2s/step - loss: 0.1910 - val_loss: 0.2836\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1911\n",
      "Epoch 85: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 115s 2s/step - loss: 0.1911 - val_loss: 0.3009\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1908\n",
      "Epoch 86: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 115s 2s/step - loss: 0.1908 - val_loss: 0.2911\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1911\n",
      "Epoch 87: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 116s 2s/step - loss: 0.1911 - val_loss: 0.2895\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1910\n",
      "Epoch 88: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1910 - val_loss: 0.2897\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1905\n",
      "Epoch 89: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1905 - val_loss: 0.2920\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1901\n",
      "Epoch 90: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1901 - val_loss: 0.3026\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1897\n",
      "Epoch 91: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1897 - val_loss: 0.2954\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1899\n",
      "Epoch 92: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 108s 1s/step - loss: 0.1899 - val_loss: 0.2980\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1898\n",
      "Epoch 93: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1898 - val_loss: 0.2958\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1894\n",
      "Epoch 94: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1894 - val_loss: 0.2946\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1898\n",
      "Epoch 95: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1898 - val_loss: 0.2917\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1890\n",
      "Epoch 96: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1890 - val_loss: 0.3053\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1892\n",
      "Epoch 97: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 109s 1s/step - loss: 0.1892 - val_loss: 0.2976\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1891\n",
      "Epoch 98: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 110s 1s/step - loss: 0.1891 - val_loss: 0.2927\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1891\n",
      "Epoch 99: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1891 - val_loss: 0.2986\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1888\n",
      "Epoch 100: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1888 - val_loss: 0.2975\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1890\n",
      "Epoch 101: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 110s 1s/step - loss: 0.1890 - val_loss: 0.3006\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 102: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1883 - val_loss: 0.2925\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1884\n",
      "Epoch 103: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1884 - val_loss: 0.2941\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 104: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1879 - val_loss: 0.3011\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 105: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 109s 1s/step - loss: 0.1879 - val_loss: 0.2913\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 106: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1879 - val_loss: 0.2951\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1885\n",
      "Epoch 107: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 109s 1s/step - loss: 0.1885 - val_loss: 0.2995\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1889\n",
      "Epoch 108: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 110s 1s/step - loss: 0.1889 - val_loss: 0.3124\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1876\n",
      "Epoch 109: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1876 - val_loss: 0.2926\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1872\n",
      "Epoch 110: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 114s 1s/step - loss: 0.1872 - val_loss: 0.2906\n",
      "Epoch 111/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1875\n",
      "Epoch 111: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1875 - val_loss: 0.3013\n",
      "Epoch 112/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 112: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 115s 2s/step - loss: 0.1873 - val_loss: 0.3042\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 113: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 106s 1s/step - loss: 0.1873 - val_loss: 0.3014\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 114: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 112s 1s/step - loss: 0.1866 - val_loss: 0.2932\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1871\n",
      "Epoch 115: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 110s 1s/step - loss: 0.1871 - val_loss: 0.2883\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 116: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 107s 1s/step - loss: 0.1866 - val_loss: 0.3010\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 117: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1863 - val_loss: 0.3097\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1869\n",
      "Epoch 118: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 111s 1s/step - loss: 0.1869 - val_loss: 0.2980\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 119: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 110s 1s/step - loss: 0.1866 - val_loss: 0.2984\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1860\n",
      "Epoch 120: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1860 - val_loss: 0.2969\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 121: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 113s 1s/step - loss: 0.1862 - val_loss: 0.2993\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 122: val_loss did not improve from 0.27694\n",
      "76/76 [==============================] - 96s 1s/step - loss: 0.1862 - val_loss: 0.3004\n",
      "Epoch 123/200\n",
      " 4/76 [>.............................] - ETA: 1:15 - loss: 0.2044"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Train classification model (first phase):\n",
    "#\n",
    "optimizer = model_development.create_optimizer('nadam')\n",
    "cae_vgg19 = \\\n",
    "    model_development.cae_VGG19(input_shape = (300, 300, 3), weights = 'imagenet', freeze_convolutional_base = True, display_model_information = False)\n",
    "\n",
    "history, training_time = model_development.train_reconstruction_model(1, cae_vgg19, optimizer = optimizer,\n",
    "    training_metrics = model_development.ssim_loss, model_name = 'CAE_VGG19', version = '1.00.00_fs',\n",
    "    X = X_train, Y = X_train, \n",
    "    metric_to_monitor = 'val_loss', no_of_epochs = 200, batch_size = 16, validation_split_ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Train classification model (second phase):\n",
    "#\n",
    "optimizer = model_development.create_optimizer('adam')\n",
    "history, training_time = model_development.train_reconstruction_model(2, '../models/weights/CAE_VGG19_1.00.00_fs.hdf5',\n",
    "    optimizer = optimizer, training_metrics = model_development.ssim_loss, \n",
    "    model_name = 'CAE_VGG19', version = '1.00.00_ss',\n",
    "    X = X_train, Y = X_train, \n",
    "    metric_to_monitor = 'val_loss', no_of_epochs = 200, batch_size = 16, validation_split_ratio = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorflow_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d1c19e37e5f6196bc516d7b965f651cc74a678b65e0e3f127f9e9132621bafc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
