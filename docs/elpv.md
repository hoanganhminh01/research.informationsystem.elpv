\documentclass[sn-mathphys]{sn-jnl}
\usepackage{multirow}
\jyear{2022}%
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%
\newtheorem{proposition}[theorem]{Proposition}%
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom

\begin{document}

\title[Utilising Deep Learning Models for Fault Detection and Diagnosis]{Utilising Deep Learning Models for Fault Detection and Diagnosis of Photovoltaic Modules}

\author*[1]{\fnm{Quang-Nguyen} \sur{Vo-Huynh}}\email{huynh.quang.nguyen.vo@intel.com}

\author[2]{\fnm{Khoa} \sur{Nguyen-Minh}}\email{khoa.nguyen@aalto.fi}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Minh} \sur{A. Hoang}}\email{minh257@cs.washington.edu}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Assembly Engineering}, \orgname{Intel Products Vietnam}, \orgaddress{\street{Lot I2, D1 Street, Tan Phu}, \city{Thu Duc City}, \postcode{70000}, \state{HCMC}, \country{Vietnam}}}

\affil[2]{\orgdiv{School of Science}, \orgname{Aalto University}, \orgaddress{\street{Otakaari 1B}, \city{Espoo}, \postcode{02150}, \state{Uusimaa}, \country{Finland}}}

\affil[3]{\orgdiv{Paul G. Allen School of Computer Science and Engineering}, \orgname{University of Washington}, \orgaddress{\street{185 E Stevens Way NE}, \city{Seattle},  \state{WA}, \postcode{98195}, \country{USA}}}

\abstract{Renewable energy sources have long been considered to be the sole alternatives for fossil fuels. Consequently, the usage of photovoltaic (PV) systems has experienced exponential growth. This growth, however, places a gargantuan pressure on the solar energy industry’s manufacturing sector and subsequently beget issues associating with the quality of PV systems -especially the PV module, which is the systems’ most crucial component. Currently, the fault detection and diagnosis (FDD) is challenging due to many factors including but not limited to requirements of sophisticated measurement instruments and experts. Recent advances in deep learning (DL) have proven its feasibility in image classification and object detection. Thus, DL can be extended to visual fault detection using data generated by electroluminescence (EL) imaging instruments. Here, we propose an in-depth approach in exploratory data analysis of EL data, as well as several techniques based on both supervised and unsupervised learnings to detect and diagnose visual faults and defects presented in a module.}

\keywords{Computer Vision, Supervised Learning, Unsupervised Learning, Deep Learning, Neural Networks}

\maketitle

\section{INTRODUCTION}\label{INTRODUCTION}
Recent years have seen a rapid increase in the use of photovoltaic (PV) electricity. Since 2010, the cumulative annual growth rate of the PV market and the global expansion of PV capacity have increased consistently at average rates of 20\% and 12\% year over year (YoY), respectively \cite{snapshot2020}. Most importantly, the increasing trend of PV capacity continues despite the COVID-19 pandemic in 2020 and 2021 \cite{snapshot2022}. The exponential expansion of PV systems suggests that the world is shifting toward renewable energy sources. However, this quick expansion places significant pressure on the solar energy industry's manufacturing sector to satisfy such high demand, resulting in several challenges related to system quality, particularly its most critical component: the PV module.

\vspace{0.25cm}
\noindent
The current method for assessing the quality of PV modules mostly comprises of failure detection and diagnosis (FDD) during the production phase, which is difficult due to a variety of issues. The most important aspect is that these flaws cannot be seen with the unaided eye. As a result, sophisticated apparatus and trained staff are necessary to make this procedure possible. Even with the availability of competent staff, errors made during the procedure can be significant. Due to the aforementioned high demand affecting the industrial sector, experienced employees are usually committed to long working hours. Thus, they are prone to blunders because of exhaustion and weariness.

\vspace{0.25cm}
\noindent
FDD processes have long been critical in a wide range of sectors, from aircraft \cite{aircraft} to automobiles \cite{automotive}, medical equipment \cite{medical} and semiconductor devices \cite{semiconductor}. The fundamental goal of the FDD process is to discover and diagnose errors as well as their related root causes early enough to allow fixes before further harm to the system or loss of service happens \cite{IGNACIOTORRENS20111258}. Since PV systems serve as power generators, failures in any component can adversely damage efficiency, energy production, security, and dependability if not detected and corrected quickly \cite{MELLIT20181}. As a result, problems must be detected correctly during the production phase to maintain optimal efficiency and energy yield while minimizing cost of maintenance and corrective operations. 

\vspace{0.25cm}
\noindent
FDD systems are classified into two types: model-based and data-driven techniques. Model-based systems incorporate domain knowledge into the system to develop a model that compares measured values of critical system parameters to reference values - commonly known as golden samples - to derive a forecast \cite{ISERMANN200571}. Data-driven models, on the other hand, have been constructed based on observations of input and output data \cite{katipamula1,katipamula2}. Thanks to the availability of massive data, substantial processing power, and the breakthrough of deep learning (DL), the employment of data-driven system has becoming increasingly appealing \cite{zhao}.

\vspace{0.25cm}
\noindent
Recent breakthroughs in machine learning (ML) approaches for image classification and pattern recognition have shown that they are completely practical for visual fault detection tasks. Few studies, however, have shown conclusive success in integrating deep learning models into the FDD process for manufacturing silicon PV modules using small datasets. As a result, the goal of our study is to propose an strategy to apply deep learning for the FDD of visual flaws on silicon PV modules in order to simplify current reliability testing utilized throughout the production phase. Our study consists of the following tasks to accomplish the objective mentioned above:    
\begin{itemize}
    \item Provide a comprehensive and thorough exploratory data analysis using a given dataset.
    \item Experiment several techniques based on both supervised and unsupervised learnings to detect and diagnose visual faults and defects presented in a module using the aforementioned dataset.
\end{itemize}

\section{RELATED WORK}\label{RELATED WORK}
\subsection{Exploratory Data Analysis}\label{Exploratory Data Analysis}
Exploratory data analysis (EDA) is a crucial procedure that entails performing early investigations on data to find patterns, identify anomalies, test hypotheses, and validate assumptions with the aid of summary statistics and graphical representations \cite{heckert2002handbook}. Despite its significant role, there is seemingly a scarcity of research on how EDA is applied in image classification. The most recent study that involves this particular subject was from Engan et al. \cite{engan}. In \cite{engan}, the authors introduced a comprehensive EDA approach consisting of two primary processes for image classification. They first computed statistical features (e.g., mean, median, standard deviation, etc. of pixels) of the images and the regions-of-interest (ROI), respectively. They then extracted the so-called ``textual" features associated with the ROI and described the former in three principal forms: statistical (calculated by the Gray Level Co-occurence Matrix), structural, and spectral. 


\subsection{Supervised Learning}\label{Supervised Learning}
Supervised Learning (SL), which is a classical field of ML, is an ML task of learning a function that maps an input to an output based on example input-output pairs \cite{brewka1996artificial}. One of the primary tasks of SL is classification - the same target of our study. 

\vspace{0.25cm}
\noindent
In recent years, there has been a rising interests in the defect detection and analysis of photovoltaic modules using electroluminescence (EL) and thermal (TM) imaging techniques. In the context of EL imaging, Deitsch et al. \cite{Deitsch2019} proposed two pipelines to determine the defect likelihood of an arbitrary solar cell. The first pipeline employed a support vector machine (SVM) to discriminate various features extracted from captured EL images. These features were extracted using different methods. In the second pipeline, the authors suggested a convolutional neural networks (CNN) to discriminate between functional and defective cells. The proposed CNN was an adaption from the VGG19 network architecture first introduced by Simonyan et al. \cite{vgg}, and it was trained with augmented data. In \cite{TANG2020453}, the authors proposed an approach of implementing CNN-based models for defect classification on an augmented dataset. This dataset was a combination traditional augmentation and image synthesis powered by generative adversarial networks (GAN) first developed by Goodfellow et al \cite{goodfellowand}. Tang et al. \cite{TANG2020453} attested that traditional augmentation was capable of generating new data with low computation time and hardware demand. Therefore, it was suitable to generate more data with simple image manipulation such as rotation, translation, and scaling. On the other hand, GAN-based image generation could potentially introduce more variations to the current dataset. As a result, the combination of these would improve the classification accuracy significantly. In a study by Bartler et al. \cite{bartler}, the authors proposed a deep learning-based classification pipeline consisting of: (1) an image pre-processing scheme for distortion correction, segmentation and perspective correction; (2) an CNN model modified from the original VGG16 network architecture \cite{vgg}. Additionally, they strongly addressed the general problem of data imbalance in automated classification of solar cell images and derived a combined method of non-heuristic re-sampling \cite{garcia2010exploring} and data augmentation to ameliorate this shortcoming.

\vspace{0.25cm}
\noindent
Considering the technique of TM, Haque et al. \cite{haque} proposed a monitoring tool for PV modules using TM and artificially intelligent systems to detect various fault types in PV modules. The monitoring tool comprised two crucial components. The first one is a function built under the principles of Discrete Wavelet Transform (DWT) \cite{graps1995introduction} to extract features from TM images and the modules' current-voltage (I-V) characteristic curve \cite{ENEBISH1993201}. The other is a multilayer perception (MLP) neural network \cite{hastie2009elements} to identify the type and the location of occurring faults using the features from TM images and I-V curves as inputs. Henry et al. \cite{henry}, on the other hand, introduced an approach inspired by the current inspection method for PV power station: using a drone with attached thermal camera. Their work implemented a deep learning-based system capable of detecting and estimating the exact location of the faulty modules among many of those in the power station. They also proposed an automatic drone flight path planning algorithm to eliminate the requirement of manual drone control. In a similar fashion, de Oliveira et al. \cite{Oliveira} proposed a method to automatically detect faults in PV arrays through aerial TM images by combining digital image processing (DIP) and CNN. Using DIP, they developed an algorithm to segment the fault area from a PV string's captured image. Then, they applied the CNN to detect faults on the aerial infrared TM images and classify them in three categories: disconnected sub-strings, hot spots, and disconnected strings.

\subsection{Unsupervised Learning}\label{Unsupervised Learning}

\section{METHOD}\label{METHOD}
\subsection{Dataset}\label{Dataset}
We evaluate our proposed EDA, supervised learning, and unsupervised learning approaches on the dataset that is publicly available from Buerhop-Lutz et al. \cite{Buerhop2018, Deitsch2021, Deitsch2019}. We denote this dataset as the ELPV dataset.

\vspace{0.25cm}
\noindent
The ELPV dataset includes 2,624 samples of 300$\times$300 pixels 8-bit grayscale images of functional (a.k.a. good) and defective (a.k.a. bad) solar cells with variable degrees of degradation. The samples were collected by segmenting and extracting individual solar cells from 44 different PV modules, 18 of which are monocrystalline and the rest are polycrystalline. Experts annotated each image in this dataset with a defect probability (a floating-point number between 0.0 and 1.0) and the type of solar module (monocrystalline or polycrystalline) from which the solar cell image was retrieved. Table \ref{elpv_dataset} provides the distribution of total number of solar cells according to defect probability and wafer type.

\begin{table}[h]
\begin{center}
\begin{tabular}{llllll}
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Solar Wafer}}} & \multicolumn{4}{c}{\textbf{Label}}                            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Total}}} \\ \cline{2-5}
\multicolumn{1}{c}{}                                      & \textbf{0\%} & \textbf{33\%} & \textbf{67\%} & \textbf{100\%} & \multicolumn{1}{c}{}                                \\ \hline
Monocrystalline                                           & 588          & 117           & 56            & 313            & 1074                                                \\ \hline
Polycrystalline                                           & 920          & 178           & 50            & 402            & 1550                                                \\ \hline
Total                                                     & 1508         & 295           & 106           & 705            & 2624                                               
\end{tabular}
\caption{Distribution of data points in the dataset from Buerhop-Lutz and Deitsch et al. according to wafer type and defect probability.}\label{elpv_dataset}
\end{center}
\end{table}

\noindent
We denote samples that are annotated with the defective probability of 33\% and 67\% as marginally defective, and samples annotated with the probability of 100\% as defective. Meanwhile, samples annotated with 0\% probability are denoted as functional. Since there are two wafer types (monocrystalline and polycrystalline), we have in total six groups consisting of mono-functional, mono-marginally-defective, mono-defective, poly-functional, poly-marginally-defective, and poly-defective.

\subsection{Exploratory Data Analysis}\label{Exploratory Data Analysis}
We propose the following hollistic approach for EDA to gain an in-depth insights on the given dataset. Firstly, we calculate a number of statistical parameters including mean, median, standard deviation (stdev), and mode image-by-image. Considering that an image is a 2-D function of intensity $I(x,y) = \{i_1(x,y), i_2(x,y), ..., i_j(x,y), ..., i_n(x,y) \}$ where $i_j(x,y)$ represents the intensity at a particular pixel and $n$ is the total number of pixels in an image, we derive the following methods to compute these statistical parameters:

\begin{align}
\centering
    &\text{Mean of pixels:} \quad \text{mn} = \frac{1}{n}\sum_{j = 1}^ni_j(x,y) \\
    &\text{Median of pixels:} \quad \text{md} = \frac{i_{j = \frac{n}{2}}(x,y) + i_{j = \frac{n+1}{2}}(x,y)}{2} \\
    &\text{Stdev of pixels:} \quad \text{std} = \sum_{j = 1}^n\frac{(i_j(x,y) - mn)^2}{n-1} \\
    &\text{Maximum of pixels:} \quad p_{max} = i_{\text{max}}(x,y) \quad \forall i_j(x,y) \in I(x,y)\\
    &\text{Minimum of pixels:} \quad p_{min} = i_{\text{min}}(x,y) \quad \forall i_j(x,y) \in I(x,y) \\
    &\text{Mode of pixels:} \quad p_{mode} = i_{\text{mode}(x,y)} \quad \forall i_j(x,y) \in I(x,y)
\end{align}

\noindent
Secondly, we compute the average image, which is calculated by taking the summation of all images associated with a specific group then dividing by the number of observations. Denoting $N$ as the number of observations, $k$ as an index of an observation $\text{I}^{\{k\}}(x,y)$ in a particular group, we derive the following method to compute an average image from samples of a specific group:

\begin{equation}
    \bar{\text{I}}(x,y) = \frac{1}{N}\sum_{k=1}^N\text{I}^{\{k\}}(x,y) 
\end{equation}

\noindent
Since the average images are the representative of their respective groups, we then leverage them to compute the difference, a.k.a. the contrast between good and bad samples. For instance, we can compute the contrast between good and bad samples of monocrystalline wafer as follows:

\begin{align}
    C_{1}(x,y) &= \bar{\text{I}}_{\text{functional}}(x,y) - \bar{\text{I}}_{\text{marginally defective}}(x,y) \\
    C_{2}(x,y) &= \bar{\text{I}}_{\text{functional}}(x,y) - \bar{\text{I}}_{\text{defective}}(x,y)
\end{align}

\noindent
Finally, we employ both the principal component analysis (PCA) and the t-distributed stochastic neighbor embedding (t-SNE) to visualize the distribution of the entire dataset.

\subsection{Data Preparation}\label{Data Preparation}
To prepare the dataset for our study, we treat cell images annotated with the defective probability of 67\% and of 33\% (a.k.a. cell image that are classified as marginally defective by the experts) as fully defective. In other words, we annotate those images with the defective probability of 100\%. Thus, the dataset will have $295 + 106 + 705 = 1106$ defective samples and 1508 functional samples. Evidently, the distribution of samples between these two classes is relatively even since the ratio between the number of functional samples of and of defective samples is approximately 1.36. Therefore, we can safely assume that issues related to data imbalance would be unlikely to occur.

\vspace{0.25cm}
\noindent
After the defective probability re-annotation, we then partition the dataset into two 85:15 train versus test subsets, respectively. Next, we split the train subset further into two smaller 85:15 train versus validation subsets. As a result, we have 1,895 training samples, 335 validation samples, and finally 394 test samples. 

\vspace{0.25cm}
\noindent
Additionally, we introduce data augmentation to generate slightly perturbed samples to expand this dataset artificially. The augmentation variability is kept in modest because the segmented cells vary only by a few pixels along the translation axes. In this work, we apply random flips along the vertical and horizontal axes. Because the cell' busbar can be laid out either vertically or horizontally, we include the 90$^{\circ}$ random rotation. We also introduced random vertical and horizontal shifts, and zooming that are limited to $\pm$5\% of the cell dimensions (height vs. width).

\begin{figure}
    \centering
    \includegraphics[width = 1.0\textwidth]{figures/data_augmentation.png}
    \caption{Effects of our proposed data augmentation on the dataset. (i) Anatomy of a solar wafer; (ii) Results of random flips along the translation horizontal and vertical axes; (iii) Results of random horizontal and vertical shifts; (iv) Results of random zoom along the translation axes; (v) Results of random rotation by 90$^{\circ}$.}
    \label{fig:my_label}
\end{figure}

\subsection{Deep Learning Models}
To achieve the best possible results on the given dataset, we employ the means of transfer learning and the so-called ``caviar'' strategy first proposed by Prof. Andrew Ng. The ``caviar'' strategy refers to a practice of developing and training multiple models, and select the model having the best learning curve. Following this strategy, we propose eight models, five of which are supervised learning models and the rest are unsupervised learning models.

\subsubsection{Supervised Learning}\label{Supervised Learning}
\noindent
The model training consists of two phases. First, we train our models, which are loaded with the pretrained ImageNet weights \cite{imagenet}, in a transer-learning manner by only training the fully connected layers (a.k.a. Dense layers) with randomly initialized weights while keeping the
weights of the convolutional bases immutable. In the second phase, we reﬁne the weights of all layers. We employ the Adam optimizer \cite{adam} with a learning rate of $\eta = 10^{-5}$, exponential decay rates of $\beta_1 = 0.9$ and $\beta_2 = 0.999$, and the regularization value $\epsilon = 10^{-8}$ in both phases.

\vspace{0.25cm}
\noindent
Each model is trained and validated on a single NVIDIA GeForce RTX 3080Ti, and
the training session consists of a total 200 epochs. We process the augmented versions of our training samples in mini-batches of 4 samples. We implement our models using Keras version 2.9.0 \cite{chollet2015keras} with TensorFlow version 2.9.1 \cite{tensorflow2015-whitepaper}. When the training process is completed, we save the model with best possible performance in terms of validation accuracy for each proposed architecture.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/training_scheme.png}
    \caption{Training procedure for our proposed supervised learning models. The training consists of two phases: in the first phase, only the models' fully connected layers are trained; in the secobd phase, all models' layers are trained.}
    \label{}
\end{figure}

\subsubsection{Unsupervised Learning}\label{Unsupervised Learning}

\subsection{Evaluation Metrics}
Besides accuracy, we use other metrics to provide us with additional insights when measuring the performance of our models. These metrics are precision, recall, F1-score, underkill, overkill, AUC, and Matthews correlation coefficient (MCC, or commonly known as $\phi$ coefficient). To qualitatively assess the model performance, we propose employing class activation maps (CAMs) for SL models, and heatmaps for USL models.

\vspace{0.25cm}
\noindent
\textbf{Precision:} Precision tells us what proportion of identifications is correct. It is defined as the number of observations that are correctly classified as defective (true positive) over the total number of observation classified as defective by the model (total positive). Noted that total positive includes true positive observations and those that are incorrectly classified as defective (false negative). As an example, a model has a precision of 80\% when it predicts a cell to be defective means that it is correct 80\% of the time. 
\begin{equation}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}

\vspace{0.25cm}
\noindent
\textbf{Recall:} Recall - commonly known as sensitivity - represents what proportion of actual defective observation was identified correctly. It is defined as the number of observation correctly classified as defective (true positive) over the total number of relevant observations. Noted that the total number of relevant observation consists of true positive and false negative instances, and false negative is defined as the number of observations incorrectly classified as functional. Thus, a model has a recall of 55\% meaning it correctly identifies 55\% of all defective cells.

\begin{equation}
    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\vspace{0.25cm}
\noindent
\textbf{F1-score:} F1-score is the harmonic mean of precision and recall metrics.

\begin{equation}
    \text{F1-score} = \frac{1}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}}
\end{equation}

\vspace{0.25cm}
\noindent
\textbf{Underkill} and \textbf{Overkill:} In defect inspection, underkill (a.k.a. under  rejection rate) is defined as the number of occurrences a system misses detecting defective products or items. Hence, underkill is computed as the number of false negative instances over the total number of predictions. In contrast, overkill is defined as the number of occurrences a system flags products or items as defective when they are actually not. Therefore, it is computed as the number of false positive over the total number of predictions. 

\begin{align}
    \text{Underkill} &= \frac{\text{FN}}{\text{TP} + \text{FP} + \text{TN} + \text{FN}} \\
    \text{Overkill} &= \frac{\text{FP}}{\text{TP} + \text{FP} + \text{TN} + \text{FN}}
\end{align}

\vspace{0.25cm}
\noindent
\textbf{AUC:} This metric is defined as the area under the receiver operating characteristic (ROC) curve, which represents the diagnostic ability of a classifcation system as its discrimination threshold is varied.

\vspace{0.25cm}
\noindent
\textbf{Matthews correlation coefficient:} This coefficient essentially represents the correlation between predicted observations and their respective ground truths. It is considered to be one of the best measures of a classification system because it addresses well for even extremely imbalanced dataset.

\begin{equation}
    \text{MCC} = \frac{\text{TP}\times\text{TN} - \text{FP}\times\text{FN}}{\sqrt{
    (\text{TP} + \text{FP})
    (\text{TP + \text{FN}})
    (\text{TN} + \text{FP})
    (\text{TN} + \text{FN})
    }}
\end{equation}

\vspace{0.25cm}
\noindent
\textbf{CAM} and \textbf{Heatmap:} In SL, CAM is the visualization of discrimitive regions a CNN model uses to identify a specific class of a given image. A high performance CNN model generally has discriminative regions that are similar to those used by domain experts to classify an image to its corresponding class. In USL, on the other hand, heatmap is essentially a contrast image visualizing the difference between a given image and its corresponding reconstructed version.


\section{RESULTS}\label{RESULTS}
\subsection{Exploratory Data Analysis}
Figure \ref{stats_mono_functional}, \ref{stats_mono_marginally_defective}, and \ref{stats_mono_defective} illustrate the distribution of computed statisitical parameters of monocrystalline samples. Similarly, the distribution of these parameters for polycrystalline samples are represented in Figure \ref{stats_poly_functional}, \ref{stats_poly_marginally_defective}, and \ref{stats_poly_defective}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_mono_functional.png}
\caption{Distribution of the computed image-by-image statistical parameters of monocrystalline samples that are denoted as functional.}\label{stats_mono_functional}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_mono_marginally_defective.png}
\caption{Distribution of the computed image-by-image statistical parameters of monocrystalline samples that are denoted as marginally defective.}\label{stats_mono_marginally_defective}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_mono_defective.png}
\caption{Distribution of the computed image-by-image statistical parameters of monocrystalline samples that are denoted as defective.}\label{stats_mono_defective}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_poly_functional.png}
\caption{Distribution of the computed image-by-image statistical parameters of polycrystalline samples that are denoted as functional.}\label{stats_poly_functional}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_poly_marginally_defective.png}
\caption{Distribution of the computed image-by-image statistical parameters of polycrystalline samples that are denoted as marginally defective.}\label{stats_poly_marginally_defective}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/stats_poly_defective.png}
\caption{Distribution of the computed image-by-image statistical parameters of polycrystalline samples that are denoted as defective.}\label{stats_poly_defective}
\end{figure}

\vspace{0.25cm}
\noindent
The average images of all six groups (mono-functional, mono-marginally-defective, mono-defective, poly-functional, poly-marginally-defective, and poly-defective) are represented in Figure \ref{average_mono} and Figure \ref{average_poly}, respectively. The results demonstrate that in average, there is relatively no visual difference between functional, marginally-defective, and defective groups among the mono- and polycrystalline samples, respectively.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/average_mono.png}
\caption{Computed average images of monocrystalline samples according to their denotation.}\label{average_mono}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/average_poly.png}
\caption{Computed average images of polycrystalline samples according to their denotation.}\label{average_poly}
\end{figure}

\vspace{0.25cm}
\noindent
Figure \ref{contrast_mono} and \ref{contrast_poly} illustrate the dissimilarity between functional, marginally defective, and defective samples grouped by cell types. For monocrystalline samples, regions that are associated with the bus bars display the most deviation. Thus, we theorize that defects that occur in monocrysalline solar cells are commonly materialized around the bus bars. On the other hand, most deviations between good and bad polycrystalline samples are located in the right corners or both left and sides of the cell. Hence, we assume that these regions are usually plagued with damages, deformities, and faults.    

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/contrast_mono.png}
\caption{Computed contrast images among monocrystalline samples.}\label{contrast_mono}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/contrast_poly.png}
\caption{Computed contrast images among polycrystalline samples.}\label{contrast_poly}
\end{figure}

\vspace{0.25cm}
\noindent
The distributions of dataset after embedding by means of PCA and t-SNE are illustrated in Figure \ref{distribution_pca} and \ref{distribution_tSNE}, respectively. According to these figures, there is no clear distinction between each group. Therefore, we can assume that defect detection and classification using classical methods (e.g., SVM, K-NN, etc.) on image features (e.g., edges, blobs, etc.) is unfeasible.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/distribution_pca.png}
\caption{PCA visualization of the dataset's embeddings distribution. The sample classes are designated as follows: 1 stands for mono-functional; 2 mono-marginally-defective; 3 mono-defective; 4 poly-functional; 5 poly-marginally-defective; 6 poly-defective.}\label{distribution_pca}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/distribution_tSNE.png}
\caption{t-SNE visualization of the dataset's embeddings distribution. The sample classes are designated as follows: 1 stands for mono-functional; 2 mono-marginally-defective; 3 mono-defective; 4 poly-functional; 5 poly-marginally-defective; 6 poly-defective.}\label{distribution_tSNE}
\end{figure}

\section{DISCUSSION}\label{DISCUSSION}

\section{Algorithms, Program codes and Listings}\label{sec7}

Packages \verb+algorithm+, \verb+algorithmicx+ and \verb+algpseudocode+ are used for setting algorithms in \LaTeX\ using the format:

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%
\bigskip
\begin{verbatim}
\begin{algorithm}
\caption{<alg-caption>}\label{<alg-label>}
\begin{algorithmic}[1]
. . .
\end{algorithmic}
\end{algorithm}
\end{verbatim}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%

You may refer above listed package documentations for more details before setting \verb+algorithm+ environment. For program codes, the ``program'' package is required and the command to be used is \verb+\begin{program}+ \verb+...+ \verb+\end{program}+. A fast exponentiation procedure:

\begin{program}
\BEGIN \\ %
  \FOR i:=1 \TO 10 \STEP 1 \DO
     |expt|(2,i); \\ |newline|() \OD %
\rcomment{Comments will be set flush to the right margin}
\WHERE
\PROC |expt|(x,n) \BODY
          z:=1;
          \DO \IF n=0 \THEN \EXIT \FI;
             \DO \IF |odd|(n) \THEN \EXIT \FI;
\COMMENT{This is a comment statement};
                n:=n/2; x:=x*x \OD;
             \{ n>0 \};
             n:=n-1; z:=z*x \OD;
          |print|(z) \ENDPROC
\END
\end{program}


\begin{algorithm}
\caption{Calculate $y = x^n$}\label{algo1}
\begin{algorithmic}[1]
\Require $n \geq 0 \vee x \neq 0$
\Ensure $y = x^n$ 
\State $y \Leftarrow 1$
\If{$n < 0$}\label{algln2}
        \State $X \Leftarrow 1 / x$
        \State $N \Leftarrow -n$
\Else
        \State $X \Leftarrow x$
        \State $N \Leftarrow n$
\EndIf
\While{$N \neq 0$}
        \If{$N$ is even}
            \State $X \Leftarrow X \times X$
            \State $N \Leftarrow N / 2$
        \Else[$N$ is odd]
            \State $y \Leftarrow y \times X$
            \State $N \Leftarrow N - 1$
        \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%

Similarly, for \verb+listings+, use the \verb+listings+ package. \verb+\begin{lstlisting}+ \verb+...+ \verb+\end{lstlisting}+ is used to set environments similar to \verb+verbatim+ environment. Refer to the \verb+lstlisting+ package documentation for more details.

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%
\bigskip
\begin{minipage}{\hsize}%
\lstset{frame=single,framexleftmargin=-1pt,framexrightmargin=-17pt,framesep=12pt,linewidth=0.98\textwidth,language=pascal}% Set your language (you can change the language for each code-block optionally)
%%% Start your code-block
\begin{lstlisting}
for i:=maxint to 0 do
begin
{ do nothing }
end;
Write('Case insensitive ');
Write('Pascal keywords.');
\end{lstlisting}
\end{minipage}

\section{Cross referencing}\label{sec8}

\section{Examples for theorem like environments}\label{sec10}

For theorem like environments, we require \verb+amsthm+ package. There are three types of predefined theorem styles exists---\verb+thmstyleone+, \verb+thmstyletwo+ and \verb+thmstylethree+ 

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%
\bigskip
\begin{tabular}{|l|p{19pc}|}
\hline
\verb+thmstyleone+ & Numbered, theorem head in bold font and theorem text in italic style \\\hline
\verb+thmstyletwo+ & Numbered, theorem head in roman font and theorem text in italic style \\\hline
\verb+thmstylethree+ & Numbered, theorem head in bold font and theorem text in roman style \\\hline
\end{tabular}
\bigskip
%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. please ignore this.       %%
%%=============================================%%

For mathematics journals, theorem styles can be included as shown in the following examples:

\begin{theorem}[Theorem subhead]\label{thm1}
Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
Example theorem text. 
\end{theorem}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{proposition}
Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
\end{proposition}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{example}
Phasellus adipiscing semper elit. Proin fermentum massa
ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
\end{example}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{remark}
Phasellus adipiscing semper elit. Proin fermentum massa
ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
\end{remark}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{definition}[Definition sub head]
Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. 
\end{definition}

Additionally a predefined ``proof'' environment is available: \verb+\begin{proof}+ \verb+...+ \verb+\end{proof}+. This prints a ``Proof'' head in italic font style and the ``body text'' in roman font style with an open square at the end of each proof environment. 

\begin{proof}
Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
\end{proof}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

\begin{proof}[Proof of Theorem~{\upshape\ref{thm1}}]
Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
\end{proof}

\noindent
For a quote environment, use \verb+\begin{quote}...\end{quote}+
\begin{quote}
Quoted text example. Aliquam porttitor quam a lacus. Praesent vel arcu ut tortor cursus volutpat. In vitae pede quis diam bibendum placerat. Fusce elementum
convallis neque. Sed dolor orci, scelerisque ac, dapibus nec, ultricies ut, mi. Duis nec dui quis leo sagittis commodo.
\end{quote}

Sample body text. Sample body text. Sample body text. Sample body text. Sample body text (refer Figure~\ref{fig1}). Sample body text. Sample body text. Sample body text (refer Table~\ref{tab3}). 

\section{Methods}\label{sec11}

Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work. Authors are encouraged to include RIIDs where appropriate. 

\textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 

\begin{enumerate}[1.]
\item Approval: a statement which confirms that all experimental protocols were approved by a named institutional and/or licensing committee. Please identify the approving body in the methods section

\item Accordance: a statement explicitly saying that the methods were carried out in accordance with the relevant guidelines and regulations

\item Informed consent (for experiments involving humans or human tissue samples): include a statement confirming that informed consent was obtained from all participants and/or their legal guardian/s
\end{enumerate}

If your manuscript includes potentially identifying patient/participant information, or if it describes human transplantation research, or if it reports results of a clinical trial then  additional information will be required. Please visit (\url{https://www.nature.com/nature-research/editorial-policies}) for Nature Portfolio journals, (\url{https://www.springer.com/gp/authors-editors/journal-author/journal-author-helpdesk/publishing-ethics/14214}) for Springer Nature journals, or (\url{https://www.biomedcentral.com/getpublished/editorial-policies\#ethics+and+consent}) for BMC.



\section{Conclusion}\label{sec13}

Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgments}

Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval 
\item Consent to participate
\item Consent for publication
\item Availability of data and materials
\item Code availability 
\item Authors' contributions
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


%% Default %%
%%\input sn-sample-bib.tex%

\end{document}
